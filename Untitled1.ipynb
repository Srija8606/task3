{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3266445a-668a-4d11-bbd3-7ccbbef52676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500 cat files, 12500 dog files\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "traindir = r\"C:\\Users\\Srija\\Downloads\\train\\train\"\n",
    "cat_files = sorted([f for f in os.listdir(traindir) if f.startswith(\"cat\")])\n",
    "dog_files = sorted([f for f in os.listdir(traindir) if f.startswith(\"dog\")])\n",
    "print(f\"{len(cat_files)} cat files, {len(dog_files)} dog files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0132e9c2-878f-445f-8854-59b0b75025dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded batch: 500 cats, 500 dogs\n",
      "[INFO] Loaded PCA.\n",
      "[INFO] Loaded model checkpoint.\n",
      "[INFO] Trained & saved model checkpoint for this batch.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# import joblib\n",
    "# import os\n",
    "\n",
    "# # ---------- PARAMETERS TO CHANGE FOR EACH NEW BATCH ---------------\n",
    "# start_cat = 0        # set to 0, then 1000, then 2000, etc. for each run\n",
    "# start_dog = 0\n",
    "# batch_size = 1000\n",
    "# img_size = 64\n",
    "# traindir = r\"C:\\Users\\Srija\\Downloads\\train\\train\"\n",
    "\n",
    "# # Reload file list, so notebook can use result from Cell 2:\n",
    "# cat_files = sorted([f for f in os.listdir(traindir) if f.startswith('cat')])\n",
    "# dog_files = sorted([f for f in os.listdir(traindir) if f.startswith('dog')])\n",
    "\n",
    "# # Select current batch\n",
    "# cat_batch_files = cat_files[start_cat:start_cat+batch_size]\n",
    "# dog_batch_files = dog_files[start_dog:start_dog+batch_size]\n",
    "\n",
    "# # Helper, loads images given the sliced list\n",
    "# def load_images(files, label):\n",
    "#     data, labels = [], []\n",
    "#     for file in files:\n",
    "#         try:\n",
    "#             img = cv2.imread(os.path.join(traindir, file), cv2.IMREAD_GRAYSCALE)\n",
    "#             img = cv2.resize(img, (img_size, img_size))\n",
    "#             data.append(img.flatten())\n",
    "#             labels.append(label)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error reading {file}: {e}\")\n",
    "#     return data, labels\n",
    "\n",
    "# # Load actual images and labels\n",
    "# x_cat, y_cat = load_images(cat_batch_files, 0)\n",
    "# x_dog, y_dog = load_images(dog_batch_files, 1)\n",
    "\n",
    "# x = np.array(x_cat + x_dog)\n",
    "# y = np.array(y_cat + y_dog)\n",
    "# print(f\"Loaded batch: {len(x_cat)} cats, {len(x_dog)} dogs\")\n",
    "\n",
    "# # Reload PCA/make new if first time\n",
    "# pca_path = \"pca_catsdogs.joblib\"\n",
    "# if os.path.exists(pca_path):\n",
    "#     pca = joblib.load(pca_path)\n",
    "#     print(\"[INFO] Loaded PCA.\")\n",
    "# else:\n",
    "#     # Fit PCA only ONCE on first batch, then use for all\n",
    "#     pca = PCA(n_components=n_components, whiten=True)\n",
    "#     pca.fit(x / 255.0)\n",
    "#     joblib.dump(pca, pca_path)\n",
    "#     print(\"[INFO] Fitted & saved PCA.\")\n",
    "\n",
    "# x_pca = pca.transform(x / 255.0)\n",
    "\n",
    "# # Reload Checkpointed model or Create if first run\n",
    "# model_path = \"sgd_catsdogs.joblib\"\n",
    "# if os.path.exists(model_path):\n",
    "#     sgd = joblib.load(model_path)\n",
    "#     print(\"[INFO] Loaded model checkpoint.\")\n",
    "#     classes_supplied = False\n",
    "# else:\n",
    "#     sgd = SGDClassifier(loss='log_loss', max_iter=1, learning_rate='optimal', warm_start=True)\n",
    "#     classes_supplied = True\n",
    "\n",
    "# # Train on this batch\n",
    "# if classes_supplied:\n",
    "#     sgd.partial_fit(x_pca, y, classes=[0,1])\n",
    "# else:\n",
    "#     sgd.partial_fit(x_pca, y)\n",
    "\n",
    "# joblib.dump(sgd, model_path)\n",
    "# print(\"[INFO] Trained & saved model checkpoint for this batch.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c932ac7-bf74-402c-b8f1-db34c00e1f6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(cat_files[start_index:start_index\u001b[38;5;241m+\u001b[39mbatch_size], [\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39mbatch_size)) \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(dog_files[start_index:start_index\u001b[38;5;241m+\u001b[39mbatch_size], [\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39mbatch_size))\n\u001b[0;32m     50\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(batch)\n\u001b[1;32m---> 51\u001b[0m batch_files, batch_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m     52\u001b[0m x_batch, _ \u001b[38;5;241m=\u001b[39m load_images(batch_files, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(batch_labels)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import joblib\n",
    "import random\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# -------- PARAMETERS ----------\n",
    "start_index = 12500\n",
    "batch_size = 1000\n",
    "img_size = 64\n",
    "n_components = 100  # for PCA\n",
    "memory_size = 5000\n",
    "epochs_per_batch = 3  # we'll retrain multiple times\n",
    "traindir = r\"C:\\Users\\Srija\\Downloads\\train\\train\"\n",
    "\n",
    "model_path = \"svc_catsdogs.joblib\"\n",
    "pca_path = \"pca_incremental.joblib\"\n",
    "memory_path = \"memory_data.npz\"\n",
    "\n",
    "# -------- Validation set ----------\n",
    "val_cat = sorted([f for f in os.listdir(traindir) if f.startswith('cat')])[9000:9200]\n",
    "val_dog = sorted([f for f in os.listdir(traindir) if f.startswith('dog')])[9000:9200]\n",
    "\n",
    "def load_images(files, label):\n",
    "    X, y = [], []\n",
    "    for f in files:\n",
    "        try:\n",
    "            img = cv2.imread(os.path.join(traindir, f), cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, (img_size, img_size))\n",
    "            X.append(img.flatten())\n",
    "            y.append(label)\n",
    "        except:\n",
    "            pass\n",
    "    return X, y\n",
    "\n",
    "x_val_cat, y_val_cat = load_images(val_cat, 0)\n",
    "x_val_dog, y_val_dog = load_images(val_dog, 1)\n",
    "x_val = np.array(x_val_cat + x_val_dog)\n",
    "y_val = np.array(y_val_cat + y_val_dog)\n",
    "\n",
    "# -------- Load current batch ----------\n",
    "cat_files = sorted([f for f in os.listdir(traindir) if f.startswith('cat')])\n",
    "dog_files = sorted([f for f in os.listdir(traindir) if f.startswith('dog')])\n",
    "\n",
    "batch = list(zip(cat_files[start_index:start_index+batch_size], [0]*batch_size)) + \\\n",
    "        list(zip(dog_files[start_index:start_index+batch_size], [1]*batch_size))\n",
    "random.shuffle(batch)\n",
    "batch_files, batch_labels = zip(*batch)\n",
    "x_batch, _ = load_images(batch_files, None)\n",
    "y_batch = list(batch_labels)\n",
    "\n",
    "# -------- Memory Replay ----------\n",
    "if os.path.exists(memory_path):\n",
    "    mem = np.load(memory_path, allow_pickle=True)\n",
    "    memory_x = list(mem[\"x\"])\n",
    "    memory_y = list(mem[\"y\"])\n",
    "else:\n",
    "    memory_x, memory_y = [], []\n",
    "\n",
    "memory_x.extend(x_batch)\n",
    "memory_y.extend(y_batch)\n",
    "if len(memory_x) > memory_size:\n",
    "    idx = np.random.choice(len(memory_x), memory_size, replace=False)\n",
    "    memory_x = [memory_x[i] for i in idx]\n",
    "    memory_y = [memory_y[i] for i in idx]\n",
    "\n",
    "np.savez(memory_path, x=memory_x, y=memory_y)\n",
    "\n",
    "x_train = np.array(memory_x)\n",
    "y_train = np.array(memory_y)\n",
    "\n",
    "# -------- PCA ----------\n",
    "if os.path.exists(pca_path):\n",
    "    pca = joblib.load(pca_path)\n",
    "else:\n",
    "    pca = IncrementalPCA(n_components=n_components, whiten=True)\n",
    "    pca.partial_fit(x_train / 255.0)\n",
    "\n",
    "pca.partial_fit(x_train / 255.0)\n",
    "joblib.dump(pca, pca_path)\n",
    "\n",
    "x_train_pca = pca.transform(x_train / 255.0)\n",
    "x_val_pca = pca.transform(x_val / 255.0)\n",
    "\n",
    "# -------- SVC Training ----------\n",
    "svc = SVC(kernel='rbf', C=1.0, gamma='scale', probability=False)  # kernel SVC :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "# Repeated training for stability (optional).\n",
    "for _ in range(epochs_per_batch):\n",
    "    svc.fit(x_train_pca, y_train)\n",
    "\n",
    "# -------- Validation ----------\n",
    "y_pred = svc.predict(x_val_pca)\n",
    "val_acc = accuracy_score(y_val, y_pred)\n",
    "print(f\"[INFO] Validation Accuracy after batch {start_index}: {val_acc:.4f}\")\n",
    "\n",
    "# -------- Save Model ----------\n",
    "joblib.dump(svc, model_path)\n",
    "print(\"[INFO] SVC model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2a0de8-8975-4215-a28d-527358f34de1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
